{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "be60dbd3",
      "metadata": {
        "id": "be60dbd3"
      },
      "source": [
        "# Session 0: Orientation and setup. LLMs for low-resource languages\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "**ğŸ“š Course Repository:** [github.com/NinaKivanani/Tutorials_low-resource-llm](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NinaKivanani/Tutorials_low-resource-llm/blob/main/Session0_Orientation_and_Setup_LLMs_Low_Resource.ipynb)\n",
        "[![GitHub](https://img.shields.io/badge/GitHub-View%20Repository-blue?logo=github)](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
        "[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)\n",
        "\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "## Your Learning Quest Begins Here!\n",
        "\n",
        "**Welcome, Language Champion!** ğŸ‘‹ You're about to embark on a journey that will transform you from an AI curious learner into a **multilingual AI expert**. This isn't just another coding tutorialâ€”it's your mission to democratize AI for the world's 6,900+ languages!\n",
        "\n",
        "### ğŸ† Your 30-Minute Setup Challenge:\n",
        "```\n",
        "ğŸ¯ Mission Checklist:\n",
        "â”œâ”€â”€ ğŸ”§ Power Up Your Environment     [â–¡â–¡â–¡â–¡â–¡] 0%\n",
        "â”œâ”€â”€ ğŸŒ Choose Your Language Quest    [â–¡â–¡â–¡â–¡â–¡] 0%  \n",
        "â”œâ”€â”€ ğŸ“Š Build Your Evaluation Toolkit [â–¡â–¡â–¡â–¡â–¡] 0%\n",
        "â”œâ”€â”€ ğŸ¤– Test with Real AI Models      [â–¡â–¡â–¡â–¡â–¡] 0%\n",
        "â””â”€â”€ ğŸš€ Ready for Session 1!         [â–¡â–¡â–¡â–¡â–¡] 0%\n",
        "```\n",
        "\n",
        "**â±ï¸ Total time:** 30-45 minutes of pure setup magic!  \n",
        "\n",
        "---\n",
        "\n",
        "## ğŸŒŸ The Big Picture: Why this session matters\n",
        "\n",
        "**ğŸš¨ The Problem:\n",
        "\n",
        "Current LLMs are strongest in English and a small number of high-resource languages. For most of the worldâ€™s languages, data coverage, tools, and evaluation resources are limited.  \n",
        "\n",
        "This session helps you:\n",
        "\n",
        "- make sure your technical setup will not block you in later sessions  \n",
        "- prepare language-specific examples that you will re-use across the course  \n",
        "- think from the beginning about correctness, fluency, cultural fit, and safety in low-resource settings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c474afb1",
      "metadata": {
        "id": "c474afb1"
      },
      "source": [
        "## ğŸ—ºï¸ Tutorial roadmap. where Session 0 fits\n",
        "\n",
        "### The full tutorial is organized as follows.\n",
        "\n",
        "**ğŸ” Session 0. Orientation and setup**  \n",
        "- Environment, language choice, and evaluation sheet preparation.\n",
        "**ğŸ” Session 1: Intro to LLM: Dialogue summarization in low-resource settings**\n",
        "- How LLMs handle multilingual dialogue, tokenization effects, and simple evaluation.\n",
        "\n",
        "**ğŸ¯ Session 2: Prompt design and cross-lingual prompting**\n",
        "- Designing prompts that work across languages, with a focus on low-resource ones.\n",
        "\n",
        "**âš™ï¸ Session 3: Lightweight model adaptation**\n",
        "- Fine-tuning or parameter-efficient adaptation for specific domains or languages.\n",
        "\n",
        "**âš–ï¸ Session 4: Bias, safety, and evaluation**\n",
        "- Identifying and documenting biases and failure patterns, especially for under-represented communities.\n",
        "\n",
        "This notebook only aims to ensure that, by the time you start Session 1, your environment and data are ready."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9082f776",
      "metadata": {
        "id": "9082f776"
      },
      "source": [
        "## âœ… Step 0: Quick Start Checklist\n",
        "Before you proceed, check the following.\n",
        "\n",
        "**ğŸ”§ Environment**\n",
        "- [ ] You can run a notebook in **Google Colab** or a **local Jupyter** installation  \n",
        "- [ ] You know how to run cells from top to bottom  \n",
        "- [ ] Optional. You have access to a GPU runtime in Colab or locally\n",
        "\n",
        "\n",
        "**ğŸŒ Language choice**\n",
        "- [ ] You have chosen at least one target language (for example Luxembourgish, Irish, Welsh, Maltese, Basque, etc.)  \n",
        "- [ ] You can provide 5â€“10 short example sentences in this language  \n",
        "- [ ] The sentences contain no personal or sensitive data\n",
        "\n",
        "**ğŸ”‘ Accounts (optional but useful)**\n",
        "- [ ] [Hugging Face account](https://huggingface.co) (free model access)\n",
        "- [ ] [GitHub account](https://github.com) (save your work)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a95cf03",
      "metadata": {
        "id": "0a95cf03"
      },
      "source": [
        "---\n",
        "\n",
        "## ğŸ® Notebook Survival Guide\n",
        "\n",
        "### ğŸ”„ Essential Habits (To avoid unnecessary debugging later, follow these practices.)\n",
        "- **Run cells top-to-bottom** when starting fresh\n",
        "- **Restart runtime if weird errors appear** : If you get unexpected import or CUDA errors, **restart the runtime** and re-run from the top  \n",
        "    - Colab. `Runtime â†’ Restart runtime`\n",
        "- **Save frequently** (Ctrl+S / Cmd+S):\n",
        "  - Colab. saves automatically, but you can also download a copy  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b040d5fe",
      "metadata": {
        "id": "b040d5fe"
      },
      "source": [
        "## ğŸ”§ Step 1. Choose your execution platform\n",
        "\n",
        "You can run this notebook in any recent Jupyter environment. The most common options are.\n",
        "\n",
        "```\n",
        "ğŸ¥‡ Google Colab (Recommended)\n",
        "   â”œâ”€â”€ âœ… Zero setup required\n",
        "   â”œâ”€â”€ âœ… Free GPU power\n",
        "   â”œâ”€â”€ âœ… Works everywhere\n",
        "   â””â”€â”€ ğŸ® Click \"Open in Colab\" above!\n",
        "\n",
        "ğŸ¥ˆ Local Jupyter  \n",
        "   â”œâ”€â”€ âœ… Full control over the Python environment\n",
        "   â”œâ”€â”€ âœ… Works offline\n",
        "   â”œâ”€â”€ âš ï¸  Requires Python 3.8+\n",
        "   â””â”€â”€ ğŸ® Install packages below\n",
        "```\n",
        "\n",
        "In all cases, the next cell installs the Python libraries used in the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b14e214",
      "metadata": {
        "id": "8b14e214",
        "outputId": "4a099eb2-7a8d-4f27-9d30-9a224930a251"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15395.43s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "conda-repo-cli 1.0.75 requires requests_mock, which is not installed.\n",
            "conda-repo-cli 1.0.75 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
            "conda-repo-cli 1.0.75 requires requests==2.31.0, but you have requests 2.32.5 which is incompatible.\n",
            "anaconda-cloud-auth 0.1.4 requires pydantic<2.0, but you have pydantic 2.9.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m15466.45s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
            "tables 3.8.0 requires cython>=0.29.21, which is not installed.\n",
            "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# ğŸš€ Installing Essential AI Libraries\n",
        "#\n",
        "# What we're installing and why:\n",
        "# â€¢ transformers: Access to pre-trained language models (BERT, GPT, etc.)\n",
        "# â€¢ datasets: Loading multilingual datasets from Hugging Face\n",
        "# â€¢ sentencepiece & tokenizers: Breaking text into pieces AI can understand\n",
        "# â€¢ accelerate: Makes models run faster\n",
        "# â€¢ sentence-transformers: Converting sentences to numbers for comparison\n",
        "# â€¢ scikit-learn: Machine learning tools for evaluation\n",
        "\n",
        "print(\"ğŸ¤– Installing AI libraries (2-3 minutes)...\")\n",
        "!pip -q install transformers datasets sentencepiece tokenizers accelerate sentence-transformers scikit-learn\n",
        "print(\"âœ… Installation complete! (Dependency warnings are normal in Colab)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fba6e81d",
      "metadata": {
        "id": "fba6e81d"
      },
      "source": [
        "## ğŸ” Step 3: System Diagnostics & Power Check!\n",
        "We will.\n",
        "\n",
        "- print versions of the main libraries  \n",
        "- check whether a GPU is available  \n",
        "- confirm that basic plotting tools work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf1bdc1d",
      "metadata": {
        "id": "cf1bdc1d",
        "outputId": "aa9f6d3c-db56-42f7-b0e3-0eaf234608c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“… Setup Check Report\n",
            "========================================\n",
            "ğŸ“… Date: 2026-01-12 11:01 UTC\n",
            "ğŸ Python: 3.11.5\n",
            "ğŸ’» Platform: macOS-15.3-arm64-arm-64bit\n",
            "\n",
            "ğŸ“š Library Versions:\n",
            "  âœ… ğŸ¤— Transformers: 4.57.3\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/nina.hosseinikivanan/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  âœ… ğŸ“Š Datasets: 4.4.2\n",
            "  âœ… ğŸ”¤ Sentence Transformers: 5.2.0\n",
            "  âœ… ğŸ”¥ PyTorch: 2.9.1\n",
            "  âœ… ğŸ§  Scikit-learn: 1.8.0\n",
            "  âœ… ğŸ“ˆ Matplotlib: 3.10.8\n",
            "  âœ… ğŸ¼ Pandas: 2.3.3\n",
            "  âœ… ğŸ”¢ NumPy: 1.26.4\n",
            "\n",
            "ğŸ’¡ All libraries should show âœ… - if you see âŒ, re-run the installation cell above!\n"
          ]
        }
      ],
      "source": [
        "# ğŸ” Verifying Your Setup\n",
        "#\n",
        "# This cell checks that all libraries installed correctly and shows your system info.\n",
        "# If you see any âŒ, re-run the installation cell above.\n",
        "\n",
        "import sys, platform, importlib\n",
        "from datetime import datetime\n",
        "\n",
        "def get_version(name: str) -> str:\n",
        "    try:\n",
        "        return importlib.import_module(name).__version__\n",
        "    except:\n",
        "        return \"âŒ not available\"\n",
        "\n",
        "print(f\"ğŸ® SYSTEM CHECK - {datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC')}\")\n",
        "print(f\"ğŸ Python: {sys.version.split()[0]} | ğŸ’» Platform: {platform.system()}\")\n",
        "print()\n",
        "\n",
        "# Check essential libraries\n",
        "libraries = [\n",
        "    (\"transformers\", \"Transformers\"), (\"datasets\", \"Datasets\"),\n",
        "    (\"sentence_transformers\", \"Sentence Transformers\"), (\"torch\", \"PyTorch\"),\n",
        "    (\"sklearn\", \"Scikit-learn\"), (\"matplotlib\", \"Matplotlib\"),\n",
        "    (\"pandas\", \"Pandas\"), (\"numpy\", \"NumPy\")\n",
        "]\n",
        "\n",
        "print(\"ğŸ“š Library Status:\")\n",
        "for pkg, name in libraries:\n",
        "    version = get_version(pkg)\n",
        "    status = \"âœ…\" if \"not available\" not in version else \"âŒ\"\n",
        "    print(f\"  {status} {name}: {version}\")\n",
        "\n",
        "print(\"\\nAll should show âœ… - if you see âŒ, re-run installation above!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "006a2109",
      "metadata": {
        "id": "006a2109"
      },
      "outputs": [],
      "source": [
        "# ğŸ–¥ï¸ Checking Checking available hardware\n",
        "#\n",
        "# This cell tests whether a GPU is accessible. The course can be completed on CPU,\n",
        "# but a GPU will make later experiments faster.\n",
        "\n",
        "print(\"\\nHardware Check:\")\n",
        "try:\n",
        "    import torch\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"ğŸš€ GPU: âœ… {torch.cuda.get_device_name(0)} (CUDA {torch.version.cuda})\")\n",
        "        print(\"   ğŸ’¡ Great! You can run larger models faster\")\n",
        "    else:\n",
        "        print(\"ğŸ–¥ï¸ GPU: âŒ Not available (using CPU)\")\n",
        "        print(\"   ğŸ’¡ No worries! CPU works fine for this course\")\n",
        "        print(\"   ğŸ”§ For GPU in Colab: Runtime â†’ Change runtime type â†’ GPU\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Hardware check failed: {e}\")\n",
        "    print(\"ğŸ’¡ This might be okay - continue with the course\")\n",
        "\n",
        "print(\"\\nğŸ‰ Setup complete! Ready for your language adventure!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ab568db",
      "metadata": {
        "id": "0ab568db"
      },
      "source": [
        "## ğŸŒ Step 3: Select your target language!\n",
        "\n",
        "Throughout the tutorial we will repeatedly use a small set of sentences in at least one target language. This session helps you define that set in a structured way.\n",
        "\n",
        "### ğŸ¯ Language Selection Strategy:\n",
        "```\n",
        "ğŸ† Perfect Test Languages:\n",
        "â”œâ”€â”€ ğŸ‡±ğŸ‡º Luxembourgish (~600K speakers)\n",
        "â”œâ”€â”€ ğŸ‡®ğŸ‡ª Irish (~1.7M speakers)  \n",
        "â”œâ”€â”€ ğŸ‡²ğŸ‡¹ Maltese (~500K speakers)\n",
        "â”œâ”€â”€ ğŸ‡®ğŸ‡¸ Icelandic (~350K speakers)\n",
        "â”œâ”€â”€ ğŸ´ó §ó ¢ó ·ó ¬ó ³ó ¿ Welsh (~900K speakers)\n",
        "â””â”€â”€ ğŸ‡ªğŸ‡¸ Basque (~750K speakers)\n",
        "```\n",
        "\n",
        "When choosing a language, consider.\n",
        "\n",
        "- coverage. languages with limited web presence are particularly interesting  \n",
        "- your own familiarity. you should be able to judge correctness and style  \n",
        "- script and morphology. non-Latin scripts or rich morphology often reveal weaknesses\n",
        "\n",
        "When creating sentences, aim for.\n",
        "\n",
        "- **Length**. roughly 6â€“20 words  \n",
        "- **Content**. a mix of simple statements, questions, numbers, borrowed words, and basic punctuation  \n",
        "- **Safety**. no personal names, addresses, or sensitive information\n",
        "\n",
        "The next cell defines a small dictionary with example sentences and allows you to add your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f30058e6",
      "metadata": {
        "id": "f30058e6"
      },
      "outputs": [],
      "source": [
        "# ğŸ¯ Language dataset creation\n",
        "#\n",
        "# This cell defines your target language and a set of example sentences.\n",
        "# You should replace the example sentences with your own for the language you care about.\n",
        "\n",
        "# ğŸŒ Choose your target language (edit this line!)\n",
        "TARGET_LANG = \"lb\"  # ğŸ‡±ğŸ‡º Luxembourgish (change to your choice!)\n",
        "\n",
        "print(f\" LANGUAGE SELECTED: {TARGET_LANG}\")\n",
        "print(\" Edit the line above to change your language\")\n",
        "print()\n",
        "\n",
        "# ğŸ“š Your Test Sentences (REPLACE WITH YOUR OWN!)\n",
        "mini_texts = {\n",
        "    \"en\": [\n",
        "        \"I enjoy learning how language models process text.\",\n",
        "        \"This sentence includes a number: 2026, and a comma.\",\n",
        "        \"Short prompts can still produce complex outputs.\",\n",
        "        \"Tokenization choices can change meaning and cost.\",\n",
        "        \"We will evaluate correctness, fluency, and safety.\"\n",
        "    ],\n",
        "    \"lb\": [  # Luxembourgish examples\n",
        "        \"Ech hunn Loscht ze verstoen, wÃ©i Sproochmodeller Text verschaffen.\",\n",
        "        \"DÃ«se Saz enthÃ¤lt eng Zuel: 2026, an eng Komma.\",\n",
        "        \"Kuerz Prompte kÃ«nnen trotzdeem komplex Ã„ntwerte produzÃ©ieren.\",\n",
        "        \"TokenisÃ©ierung kann Bedeitung an KÃ¤schte beaflossen.\",\n",
        "        \"Mir evaluÃ©ieren Richtegkeet, FlÃ«ssegkeet an SÃ©cherheet.\"\n",
        "    ],\n",
        "    \"ga\": [  # Irish examples\n",
        "        \"Is maith liom foghlaim conas a phrÃ³iseÃ¡lann samhlacha teanga tÃ©acs.\",\n",
        "        \"TÃ¡ uimhir sa abairt seo: 2026, agus camÃ³g.\",\n",
        "        \"Is fÃ©idir le leid ghearr aschuir chasta a thÃ¡irgeadh fÃ³s.\",\n",
        "        \"Is fÃ©idir le roghanna tocainithe brÃ­ agus costas a athrÃº.\",\n",
        "        \"DÃ©anaimid measÃºnÃº ar chruinneas, lÃ­ofacht agus sÃ¡bhÃ¡ilteacht.\"\n",
        "    ],\n",
        "    \"mt\": [  # Maltese examples\n",
        "        \"JogÄ§obni nitgÄ§allem kif il-mudelli tal-lingwa jipproÄ‹essaw it-test.\",\n",
        "        \"Din is-sentenza tinkludi numru: 2026, u virgola.\",\n",
        "        \"Prompts qosra xorta jistgÄ§u jipproduÄ‹u outputs kumplessi.\",\n",
        "        \"L-gÄ§aÅ¼liet tat-tokenization jistgÄ§u jbiddlu t-tifsira u l-ispejjeÅ¼.\",\n",
        "        \"AÄ§na nevalwaw it-tÄ§assib, il-fluwenza u s-sigurtÃ .\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# ğŸ”§ Add your language if it's not in the examples above\n",
        "if TARGET_LANG not in mini_texts:\n",
        "    print(f\"\\n {TARGET_LANG} not in examples - please add your sentences below!\")\n",
        "    mini_texts[TARGET_LANG] = [\n",
        "        \"Add your first sentence here (6-20 words).\",\n",
        "        \"Add your second sentence with a number: 2026.\",\n",
        "        \"Add your third sentence here.\",\n",
        "        \"Add your fourth sentence here.\",\n",
        "        \"Add your fifth sentence here.\"\n",
        "    ]\n",
        "    print(\" Edit the list above to add your own sentences!\")\n",
        "else:\n",
        "    print(f\"\\n Found example sentences for {TARGET_LANG}!\")\n",
        "\n",
        "print(f\"\\nğŸ“‹ Your current dataset:\")\n",
        "print(f\"ğŸŒ Language: {TARGET_LANG}\")\n",
        "print(f\"ğŸ“Š Number of sentences: {len(mini_texts[TARGET_LANG])}\")\n",
        "print(f\"ğŸ“ Example: '{mini_texts[TARGET_LANG][0]}'\")\n",
        "\n",
        "print(f\"\\nğŸ” All your {TARGET_LANG} sentences:\")\n",
        "for i, sentence in enumerate(mini_texts[TARGET_LANG], 1):\n",
        "    print(f\"  {i}. {sentence}\")\n",
        "\n",
        "print(\"\\nğŸ’¡ Remember: You can edit these sentences anytime by modifying the cell above!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4e21639",
      "metadata": {
        "id": "c4e21639"
      },
      "source": [
        "## ğŸ“Š Step 4: Build a simple evaluation framework\n",
        "\n",
        "Time to create your secret weapon for systematic AI evaluation! For low-resource languages, standard automatic metrics are often unreliable or unavailable. To keep the analysis systematic across sessions, we will use a simple human-centric rating scheme.\n",
        "\n",
        "\n",
        "### For each model output, you will record.\n",
        "\n",
        "- **âœ… Correctness (0â€“2)**: Does the output solve the task or answer the question?\n",
        "- **ğŸ—£ï¸ Fluency (0â€“2)**: Does the text read naturally in the target language?  \n",
        "- **ğŸŒ Cultural (0â€“2)**: Does the output respect linguistic and cultural norms?\n",
        "- **ğŸ›¡ï¸ Safety (0â€“2)**: Does the output avoid harmful or inappropriate content?\n",
        "- **ğŸ“ Notes (0â€“2)**: Any additional observations about errors or interesting behaviour\n",
        "\n",
        "The next cell creates a table that combines English and your target language sentences. you will fill this table during later sessions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eefc27f9",
      "metadata": {
        "id": "eefc27f9"
      },
      "outputs": [],
      "source": [
        "# Building the evaluation sheet\n",
        "#\n",
        "# This creates a structured DataFrame with one row per input sentence.\n",
        "# You will fill in the model, prompt, ratings, and notes as you run experiments.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def create_evaluation_sheet(texts: dict, target_lang: str) -> pd.DataFrame:\n",
        "    \"\"\"Create evaluation sheet with empty columns to fill during sessions\"\"\"\n",
        "    rows = []\n",
        "\n",
        "    # Add both English and target language sentences\n",
        "    for lang in [\"en\", target_lang]:\n",
        "        if lang in texts:\n",
        "            for i, sentence in enumerate(texts[lang], 1):\n",
        "                rows.append({\n",
        "                    \"item_id\": f\"{lang}_{i}\",\n",
        "                    \"language\": lang,\n",
        "                    \"input_text\": sentence,\n",
        "                    \"task\": \"\",           # Session 1: summarization, Session 2: prompting, etc.\n",
        "                    \"model\": \"\",          # Which AI model was used\n",
        "                    \"prompt_style\": \"\",   # How you asked the AI\n",
        "                    \"output_text\": \"\",    # What the AI produced\n",
        "                    \"correctness_0to2\": \"\",  # 0=wrong, 1=mostly right, 2=perfect\n",
        "                    \"fluency_0to2\": \"\",      # 0=broken, 1=awkward, 2=natural\n",
        "                    \"cultural_0to2\": \"\",     # 0=inappropriate, 1=minor issues, 2=appropriate\n",
        "                    \"safety_0to2\": \"\",       # 0=harmful, 1=minor concerns, 2=safe\n",
        "                    \"notes\": \"\"              # Your observations\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# Create your personal evaluation sheet\n",
        "eval_df = create_evaluation_sheet(mini_texts, TARGET_LANG)\n",
        "\n",
        "print(f\"ğŸ“‹ Created evaluation sheet: {len(eval_df)} rows\")\n",
        "print(f\"ğŸŒ Languages: English + {TARGET_LANG}\")\n",
        "print(f\"ğŸ“Š Sentences per language: {len(mini_texts.get('en', []))}\")\n",
        "\n",
        "print(\"\\n Preview (you'll fill the empty columns during sessions):\")\n",
        "display(eval_df.head(6))\n",
        "\n",
        "#print(\"\\nğŸ’¡ This will be your scientific log throughout all sessions!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4510b9f2",
      "metadata": {
        "id": "4510b9f2"
      },
      "source": [
        "## ğŸ’¾ Step 5: Save the evaluation sheet\n",
        "\n",
        "We now save the evaluation table to a CSV file so that you can.\n",
        "\n",
        "- re-use it in later sessions  \n",
        "- inspect or edit it outside the notebook if necessary  \n",
        "- keep a record of your ratings and model outputs\n",
        "\n",
        "The next cell writes the file to a folder called `session0_outputs`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ce3bbaa",
      "metadata": {
        "id": "5ce3bbaa"
      },
      "outputs": [],
      "source": [
        "# Saving the evaluation sheet to disk\n",
        "#\n",
        "# The file will be written as: session0_outputs/evaluation_sheet_<TARGET_LANG>.csv\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Create output folder and save evaluation sheet\n",
        "output_dir = Path(\"session0_outputs\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "filename = f\"evaluation_sheet_{TARGET_LANG}.csv\"\n",
        "file_path = output_dir / filename\n",
        "\n",
        "eval_df.to_csv(file_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"âœ… Saved: {filename}\")\n",
        "print(f\"ğŸ“ Location: {file_path}\")\n",
        "print(f\"ğŸ“Š Contains: {len(eval_df)} rows for evaluation\")\n",
        "\n",
        "# Platform-specific download instructions\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    print(\"\\nğŸ“¥ In Colab: Files panel (left) â†’ session0_outputs â†’ right-click â†’ Download\")\n",
        "else:\n",
        "    print(f\"\\nğŸ“‚ Local path: {os.path.abspath(file_path)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4f40f0f",
      "metadata": {
        "id": "e4f40f0f"
      },
      "source": [
        "## âœ… Step 6: Optional. test a multilingual model!\n",
        "\n",
        "If your network connection and runtime allow it, you can now perform a small sanity check.\n",
        "\n",
        "### ğŸ¯ This optional step will.:\n",
        "```\n",
        "ğŸ¤– Load a multilingual sentence embedding model\n",
        "ğŸ“Š Encode your English and target language sentences  \n",
        "ğŸ“ˆ Prepare data for a simple 2D visualization\n",
        "```\n",
        "\n",
        "**âš ï¸ Network issues? Skip this test and proceed - you're still ready for Session 1!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "592f294a",
      "metadata": {
        "id": "592f294a"
      },
      "outputs": [],
      "source": [
        "# ğŸ¤– Testing with a Real Multilingual AI Model\n",
        "#\n",
        "# This downloads and tests a multilingual model on your sentences.\n",
        "# It converts sentences to numbers (embeddings) that AI can compare.\n",
        "# This confirms your setup works and gives you a preview of Session 1!\n",
        "#\n",
        "# Note: First download takes 1-2 minutes (~420MB). Skip if network issues.\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "\n",
        "print(f\"ğŸ”„ Loading multilingual model: {MODEL_NAME}\")\n",
        "print(\"â±ï¸ First time: 1-2 minutes download (~420MB)\")\n",
        "\n",
        "try:\n",
        "    model = SentenceTransformer(MODEL_NAME)\n",
        "    print(\"âœ… Model loaded successfully!\")\n",
        "\n",
        "    # Prepare sentences from both languages\n",
        "    test_texts, test_labels = [], []\n",
        "    print(f\"\\nğŸ“Š Processing sentences:\")\n",
        "\n",
        "    for lang in [\"en\", TARGET_LANG]:\n",
        "        if lang in mini_texts:\n",
        "            for sentence in mini_texts[lang]:\n",
        "                test_texts.append(sentence)\n",
        "                test_labels.append(lang)\n",
        "                print(f\"  ğŸ“ {lang}: {sentence[:50]}{'...' if len(sentence) > 50 else ''}\")\n",
        "\n",
        "    # Convert sentences to numerical representations\n",
        "    print(f\"\\nğŸ§  Converting {len(test_texts)} sentences to numbers...\")\n",
        "    embeddings = model.encode(test_texts, normalize_embeddings=True)\n",
        "\n",
        "    print(f\"âœ… Success! Each sentence â†’ {embeddings.shape[1]} numbers\")\n",
        "    print(f\"ğŸ“Š Shape: {embeddings.shape} | ğŸŒ Languages: {len(set(test_labels))}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Model loading failed: {e}\")\n",
        "    print(\"ğŸ’¡ Network issue? Skip this test - you can still continue!\")\n",
        "    print(\"ğŸ”§ Or try: Runtime â†’ Restart Runtime, then re-run from top\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dafbd8b0",
      "metadata": {
        "id": "dafbd8b0"
      },
      "source": [
        "### What PCA is doing in this plot\n",
        "\n",
        "The model turns each sentence into a vector of many numbers  \n",
        "For example 384 numbers for a single sentence  \n",
        "This is too many dimensions to show in a 2D plot  \n",
        "\n",
        "**Principal Component Analysis (PCA)** is a simple way to:\n",
        "\n",
        "- compress these high dimensional vectors into 2 numbers per sentence  \n",
        "- keep as much of the original variation as possible  \n",
        "\n",
        "You can think of it like this.\n",
        "\n",
        "- Imagine you have a 3D object, but you can only draw on a flat sheet of paper  \n",
        "- You shine a light and look at the shadow on the paper  \n",
        "- The shadow loses some detail, but you still see the main shape  \n",
        "\n",
        "Here.\n",
        "\n",
        "- the 3D object is actually a high dimensional sentence embedding  \n",
        "- the 2D shadow is the point we plot for that sentence  \n",
        "\n",
        "In the scatter plot below.\n",
        "\n",
        "- each point is one sentence  \n",
        "- points that are close together represent sentences that the model sees as similar  \n",
        "- you can check whether sentences from different languages stay separate or mix together in this 2D space  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c219b14a",
      "metadata": {
        "id": "c219b14a"
      },
      "outputs": [],
      "source": [
        "# ğŸ“ˆ Visualizing How AI \"Sees\" Your Languages\n",
        "#\n",
        "# This creates a 2D map showing how the AI model groups your sentences.\n",
        "# Sentences with similar meanings should appear close together.\n",
        "# This gives you insight into how well the model understands your language!\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if 'embeddings' in locals():\n",
        "    print(\" Creating language visualization...\")\n",
        "\n",
        "    # Reduce high-dimensional numbers to 2D for plotting\n",
        "    pca = PCA(n_components=2, random_state=42)\n",
        "    embeddings_2d = pca.fit_transform(embeddings)\n",
        "\n",
        "    # Create the visualization\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
        "\n",
        "    for i, lang in enumerate(sorted(set(test_labels))):\n",
        "        # Plot sentences from each language in different colors\n",
        "        lang_indices = [j for j, label in enumerate(test_labels) if label == lang]\n",
        "        plt.scatter(embeddings_2d[lang_indices, 0], embeddings_2d[lang_indices, 1],\n",
        "                   label=f'{lang} ({len(lang_indices)} sentences)',\n",
        "                   color=colors[i % len(colors)], s=100, alpha=0.7)\n",
        "\n",
        "    plt.title(f'ğŸŒ How AI \"Sees\" Your Languages\\n(Each dot = one sentence)', fontsize=14)\n",
        "    plt.xlabel('ğŸ“Š Dimension 1', fontsize=12)\n",
        "    plt.ylabel('ğŸ“Š Dimension 2', fontsize=12)\n",
        "    plt.legend(fontsize=11)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.figtext(0.5, 0.02, 'ğŸ’¡ Closer dots = more similar meanings to the AI',\n",
        "                ha='center', fontsize=10, style='italic')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"ğŸ‰ Visualization complete!\")\n",
        "    print(\"ğŸ” Questions you can ask yourself.\")\n",
        "    print(\"  â€¢ Do sentences from the same language cluster together?\")\n",
        "    print(\"  â€¢ Do semantically similar sentences appear in similar regions?\")\n",
        "    print(\"  â€¢ Any surprising patterns?\")\n",
        "\n",
        "else:\n",
        "    print(\"âš ï¸ Skipping visualization - model loading failed above\")\n",
        "    print(\"ğŸ’¡ This is okay - you can still continue with the course!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f2f8981",
      "metadata": {
        "id": "6f2f8981"
      },
      "source": [
        "## ğŸ†˜ Troubleshooting\n",
        "\n",
        "**ğŸ”§ Some common issues and suggested actions:**\n",
        "- **Installation errors:**\n",
        "  - Restart the runtime and re-run the installation cell  \n",
        "  - Check that you are using a recent Python version\n",
        "- **Model download failures:**\n",
        "  - Often caused by transient network problems  \n",
        "  - Try again later or skip the optional model test\n",
        "- **Out of memory errors:**\n",
        "  - Switch to a smaller model or use CPU only  \n",
        "  - In Colab, verify that you have not opened multiple notebooks in the same session\n",
        "- **Encoding or Unicode problems:**\n",
        "  - Ensure files are saved with UTF-8 encoding  \n",
        "  - Avoid mixing encodings in the same project\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dee37ce",
      "metadata": {
        "id": "9dee37ce"
      },
      "source": [
        "## ğŸ¯ Mission Complete! What's Next?\n",
        "\n",
        "### âœ… Before you move to Session 1:\n",
        "It is useful to confirm that you have.\n",
        "- [ ] At least one target language selected  \n",
        "- [ ] A set of 5â€“10 sentences in that language, saved in this notebook  \n",
        "- [ ] An evaluation sheet CSV file saved under `session0_outputs/`  \n",
        "- [ ] A rough idea of what might be challenging for your language  \n",
        "      (for example. morphology, spelling variation, code-switching)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bf43678",
      "metadata": {
        "id": "9bf43678"
      },
      "source": [
        "### ğŸ¤” Reflection and next steps\n",
        "\n",
        "You can note down short answers to the following questions.\n",
        "\n",
        "1. Which phenomena in your language do you expect LLMs to handle poorly.  \n",
        "2. Where do you anticipate prompt following to break down first.  \n",
        "3. What would count as a successful outcome for you at the end of the tutorial.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ‰ Congratulations!\n",
        "\n",
        "You've successfully completed Session 0!\n",
        "\n",
        "**ğŸš€ Ready for Session 1?** Open `Session1_Dialogue_Summarization_Low_Resource.ipynb` and let the adventure continue!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}